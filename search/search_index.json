{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#machine-learning-enhanced-computational-design-of-high-level-interlocking-puzzles","title":"Machine Learning Enhanced Computational Design of High-level Interlocking Puzzles","text":"<p>Welcome to the report website of CS184 2024 spring final project <code>Machine Learning Enhanced Computational Design of High-level Interlocking Puzzles</code>.</p>"},{"location":"#hosted-content","title":"Hosted Content","text":"<ul> <li>Milestone Status Report</li> <li>Final Report</li> </ul>"},{"location":"#team-members","title":"Team Members","text":"<ul> <li>Ruhao Tian</li> <li>Yunshen Song</li> <li>Zixuan Wan</li> <li>Zineng Tang</li> </ul>"},{"location":"final_report/","title":"Final report","text":""},{"location":"final_report/#machine-learning-enhanced-computational-design-of-high-level-interlocking-puzzles","title":"Machine Learning Enhanced Computational Design of High-level Interlocking Puzzles","text":"<p>By Ruhao Tian, Yunshen Song, Zixuan Wan, Zineng Tang</p> <p>Video: Final Project Video</p>"},{"location":"final_report/#abstract","title":"Abstract","text":"<p>Interlocking puzzles are a fascinating application of computational design, but generating complex high-level puzzles is extremely time consuming due to the large number of trials required. This paper explores accelerating the puzzle generation algorithm using machine learning. We profile the algorithm to identify the bottleneck in computing seed and block paths for each puzzle piece. To accelerate this bottleneck, we generate training data by recording the path generation process and train a machine learning model to predict high-quality paths, bypassing many expensive trials. Benchmarking shows the generation time follows a geometric distribution. Applying our ML model achieves a speedup in generating challenging puzzles like the 3D spider. With this acceleration, we can generate complex puzzles in XX minutes instead of hours, enabling more efficient iteration to optimize puzzles for manual assembly. We 3D print an example puzzle to validate that our approach produces stable, physically-realizable interlocking designs.</p>"},{"location":"final_report/#introduction","title":"Introduction","text":"<p>Interlocking puzzles are a captivating application of computational design, offering both an engaging challenge for puzzle solvers and a testbed for computer-aided design algorithms. Creating puzzles with intricate, interleaving pieces requires considering numerous geometric constraints to ensure a stable final assembly with a unique disassembly path. Optimizing puzzles to be both challenging and feasible to assemble is a complex task that requires many iterations of piece design and layout. Recent work by Song et al. [1] proposed an algorithm to generate high-level interlocking puzzles through many trials to discover piece geometries that satisfy the numerous constraints. While effective at producing complex designs, a key limitation is the runtime: generating a single puzzle can take hours due to the large number of trials required. In this work, we explore accelerating the puzzle generation algorithm using machine learning (ML). We analyze the algorithm to identify bottlenecks and find that the majority of time is spent computing \\textit{seed paths} and \\textit{block paths} that determine the geometry of each piece. To accelerate this step, we propose using ML to predict high-quality paths, bypassing a large number of expensive trial-and-error iterations. We implement the puzzle generation algorithm and profile its performance on a variety of 3D models. We measure the distribution of generation times and find it follows a geometric distribution, enabling principled benchmarking. We then instrument the algorithm to record training data consisting of seed and block paths and train an ML model to predict them. Applying our ML model to the path generation step achieves a speedup on a complex spider puzzle, reducing generation time from over 15 hours to just YY minutes. We evaluate the puzzles produced by our accelerated approach and find they maintain the key properties of the baseline algorithm. We further validate our approach by 3D printing a puzzle and physically assembling it. In summary, we present the first machine learning approach to accelerating computational interlocking puzzle design, demonstrating significant speedups on complex models. Our approach enables rapid iteration to optimize puzzles for manual assembly and opens the door to generating larger and more intricate designs. We release our code and data to facilitate future research in this area.</p>"},{"location":"final_report/#technical-approach","title":"Technical Approach","text":""},{"location":"final_report/#early-attempts-of-cuda-migration","title":"Early Attempts of CUDA Migration","text":"<p>In the very early stage of our project, we decided to implement a CUDA-accelerated puzzle generator based on the SIGGRAPH 2022 paper \"Computational Design of High-level Interlocking Puzzles\". The paper introduces a novel method to generate high-level interlocking puzzles with a variety of constraints. However, the method is time-costly and we believe that it can be significantly accelerated by parallelizing the computation on GPU.</p> <p>The method proposed by the paper is based on 'trials and errors'. It may take thousands of independent trials to generate a disassemblable puzzle. Our original idea is to move the trials to GPU and run them in parallel, which can significantly reduce the time cost.</p> <p>We believed that we could directly migrate the original C++ code to CUDA code without significant changes. However, after extensive exploration, the difficulty of CUDA programming became apparent. CUDA does not support dynamic data structures, which forced us to redirect our approach.</p> <p>The key algorithm of the proposed method heavily involves graph traversal. The original C++ code implementation consists of dynamic data structures, e.g. <code>std::vector</code> to store the intermediate results. After extensive testing, we believe that it is impractical to directly migrate the original C++ code to CUDA code given the time and resources available to us. Even if we brutally replace all the <code>std::vector</code> with fixed-size arrays, the code would become inefficient because all expansions to arrays would require copying the entire array to a new location.</p> <p>This understanding is a significant setback, but we believe GPU acceleration is still possible. Instead of directly migrating all the original C++ code to CUDA code, we now need to carefully select parts of the algorithm we want to accelerate and redesign the algorithm to fit the CUDA programming model.</p>"},{"location":"final_report/#profiling-and-bottleneck-identification","title":"Profiling and Bottleneck Identification","text":"<p>To identify our selective acceleration target, we profile the implementation of the paper in search of a bottleneck. The implementation of the paper executes on the Linux platform and our group members either use macOS or virtualized WSL on Windows, consequently, we profile both platforms to minimize the difference. We use <code>perf</code> on WSL and Xcode Instruments on macOS to profile the original C++ code.</p> <p></p> <p>Profiling results from both platforms identically show that the bottleneck of the original algorithm is computing the main path when creating a new piece, which consumes around 80% of execution time in some cases.</p> <p></p> <p>To elaborate, within each trial of the algorithm, every piece of the puzzle is created by (1) finding a small set of seed voxels <code>SeedPath</code> that is movable in the target direction, (2) adding a set of block voxels <code>BlockPath</code> to prevent movement in other directions, and (3) adding voxels with small reachability to extend the piece. According to the profiling results, (1) and (2) are the most time-consuming parts of the algorithm.</p>"},{"location":"final_report/#selective-acceleration-by-machine-learning","title":"Selective Acceleration by Machine Learning","text":""},{"location":"final_report/#algorithm","title":"Algorithm","text":"<p>The path generation process in the original algorithm can be abstracted into two steps. First, the algorithm generates six candidate paths through complicated voxel space traversal. Second, the algorithm selects the best path as the final result. The selection criteria are based on the reachability of the path and the number of voxels in the path.</p> <p></p> <p>The heuristic of the original algorithm does not guarantee a global optimal solution, e.g. could lock into each other and produce deadlocks. We believe that a machine learning model can learn the heuristic from the original algorithm and provide a better selection of paths. Specifically, we implemented a random forest model [2] and fed the path information as well as whether the final puzzle configuration is solvable, aiming to provide a better success rate for each trial and increase efficiency.</p> <p>For the puzzle generation algorithm, we attempted to implement a long short-term memory (LSTM) model [3] to predict the reachability of the path. However, the LSTM model did not perform well in our experiments. Possible reasons include the lack of training data and the complexity of the LSTM model. Unfortunately, we did not have enough time to further explore the LSTM model.</p>"},{"location":"final_report/#dataset-generation","title":"Dataset Generation","text":"<p>To gather data for training, we implemented a dataset kernel to record the generation process of <code>SeedPath</code> and <code>BlockPath</code>.</p> <p></p> <p>When the <code>SeedPath</code> generation starts, a <code>seedPathCreationSequence</code> is created to record the intermediate states of the <code>SeedPath</code>. Every time the <code>SeedPath</code> changes, the updated <code>SeedPath</code> is appended to the <code>seedPathCreationSequence</code>. The final <code>seedPathCreationSequence</code> is saved to a JSON file. The same process is applied to <code>BlockPath</code>.</p>"},{"location":"final_report/#benchmarking","title":"Benchmarking","text":"<p>Measuring the performance of the original program and our implemented acceleration requires a uniform standard. This is hard for the puzzle generator as too many random factors are involved, and each generation with the same parameter can take a significantly different amount of time. We decided to explore the probability distribution of the generation time and use it as a standard to compare the performance of the original program and our implemented acceleration.</p> <p>Intuitively, we determined that it should fit a geometric distribution since we are testing how many trials it would take for the program to output a successful build. To verify our hypothesis, we need to gather trial data, but this cannot be done alone since there are multiple shapes and each run would take all of the GPU, meaning that we can only process one shape at a time. After all, we were determined to process 2 different shapes.</p> <p></p> <p>The first shape we chose is the spider because it is the most complex and takes the longest to run, so we hope that we can spot significant improvement after implementing our machine-learning model. The second shape we chose is the 5x5x5 cube, since it was the simplest of them all and takes the shortest time to run. </p> <p>We gathered 66 different trials of spiders with an average run time of 15.9 minutes and an average trial of 277.7. We gathered 145 different trials of cubes with an average run time of 0.900 minutes and an average trial of 8.9. Below are the graphs and the statistics: </p>"},{"location":"final_report/#cube","title":"Cube","text":""},{"location":"final_report/#spider","title":"Spider","text":"<p>From the graph above, we can see that the geometric distribution is a great fit for the data. Furthermore, the variance of both shapes is approximately equal to the square of the mean. This is strong evidence that our data follows the geometric model.</p> <p>Next, we will use our approximated geometric distribution to determine the improvement of our machine-learning model.</p>"},{"location":"final_report/#results","title":"Results","text":""},{"location":"final_report/#3d-printing","title":"3D Printing","text":""},{"location":"final_report/#data-analysis","title":"Data Analysis","text":"<p>Our current model aims to improve on the number of trials to find a successful build for Spiders and Cubes by choosing plausible seed paths instead of speeding up the process of seed path. Due to the amount of time spent on optimizing the machine-learning models, we perfected the model rather late, leaving us with limited time to thoroughly test it.  Below is the data we gathered from integrating our machine learning models:</p> <p>For spiders, the average trial number is 217, and the average minutes per trial is 0.05732. The value of minutes per trial stayed roughly the same from the earlier average trial time of 15.9 minutes divided by 277.7 = 0.057 minutes per trial. We think that this doesn't necessarily show improvement and the state of the computer at runtime would play a much bigger role. The trail number also improved by an average of 60 per process, which is about 22% improvment. Even though the dataset is not big (about 30) , with such a large improvement this shows that our machine learning model is successful at improving the trial number of spiders. We found that under different systems, the same seed number would lead to different results so we would have different trial and test dataset. Therefore, we estimate the code speed improved by 22%.</p> <p>For cubes, the trial nums is 9.485. This is expected as cubes are relatively fast to finish and we don\u2019t have high hopes of improving. The running time on the other hand was consistently smaller than 0.05 per process. We used a different computer to test the cubes due to technical issues, so not much can be concluded here.</p>"},{"location":"final_report/#conclusion","title":"Conclusion","text":"<p>In this work, we present a machine learning approach to accelerate the computational design of interlocking 3D puzzles. We profile the state-of-the-art algorithm to identify performance bottlenecks in generating seed and block paths for puzzle pieces. We instrument the algorithm to gather training data and train a random forest model to predict high-quality paths, reducing the number of expensive trial-and-error iterations.</p>"},{"location":"final_report/#references","title":"References","text":"<p>[1] SIGGRAPH 2022 paper \"Computational Design of High-level Interlocking Puzzles\" ACM Digital Library</p> <p>[2] Recursive interlocking puzzles ACM Digital Library</p> <p>[3] Rigatti, Steven J. \"Random forest.\" Journal of Insurance Medicine 47.1 (2017): 31-39.</p> <p>[4] Yu, Yong, et al. \"A review of recurrent neural networks: LSTM cells and network architectures.\" Neural computation 31.7 (2019): 1235-1270.</p>"},{"location":"final_report/#contributions","title":"Contributions","text":"<p>Zixuan&amp;Yunshen: Benchmarking, result analysis, 3D printing and sanding.</p> <p>Zineng: Machine learning algorithm, improvement and integration with the puzzle algorithm.</p> <p>Ruhao: Profiling, data kernel, project design.</p>"},{"location":"milestone_status_report/","title":"Milestone status report","text":""},{"location":"milestone_status_report/#milestone-status-report-the-story-so-far","title":"Milestone Status Report: The Story So Far","text":"<p>By Ruhao Tian</p>"},{"location":"milestone_status_report/#introduction-our-original-plan","title":"Introduction: Our original plan","text":"<p>In the very early stage of our project, we decided to implement a CUDA-accelerated puzzle generator based on the SIGGRAPH 2022 paper \"Computational Design of High-level Interlocking Puzzles\". The paper introduces a novel method to generate high-level interlocking puzzles with a variety of constraints. However, the method is time-costly and, in our early experiments, it takes several minutes to generate a single puzzle. We believe that the method can be significantly accelerated by parallelizing the computation on GPU.</p> <p>The method proposed by the paper is based on 'trials and errors'. It generates a puzzle in an approach that may encounter deadlocks, and if it does, it will start another trial with some random settings. It may take thousands of independent trials to generate a disassemblable puzzle. Our key idea is to move the trials to GPU and run them in parallel, which can significantly reduce the time cost.</p>"},{"location":"milestone_status_report/#full-cuda-migration-a-dead-end","title":"Full CUDA Migration: A dead end","text":"<p>No group member of our project has experience in CUDA programming. Based on the description of CUDA, it is claimed to fully support C++ syntax and we misunderstood its meaning. We thought that we could directly migrate the original C++ code to CUDA code without significant changes. However, after several days of exploration, we discovered a serious problem with CUDA is that it does not support dynamic data structures.</p> <p>The key algorithm of the proposed method heavily involves graph traversal. The original C++ code uses a lot of dynamic data structures like <code>std::vector</code> to store the intermediate results. After extensive testing, we found that it is impossible to directly migrate the original C++ code to CUDA code. Even if we brutally replace all the <code>std::vector</code> with fixed-size arrays, the code would become inefficient because all expansions to arrays would require copying the entire array to a new location.</p> <p>This understanding is a significant setback for our project, but we believe GPU acceleration is still possible. Instead of directly migrating all the original C++ code to CUDA code, we now need to carefully select parts of the algorithm we want to accelerate and redesign the algorithm to fit the CUDA programming model.</p>"},{"location":"milestone_status_report/#selective-acceleration-identifying-the-bottleneck","title":"Selective Acceleration: Identifying the bottleneck","text":"<p>As rewriting the entire algorithm in CUDA is not feasible, we need to identify the bottleneck of the original algorithm and accelerate it. The implementation of the paper executes on Linux platform and our group members either use macOS or virtualized WSL on Windows, so we profile both platforms to minimize the difference. We use <code>perf</code> on WSL and Xcode Instruments on macOS to profile the original C++ code.</p> <p></p> <p>Profiling results from both platforms identically show that the bottleneck of the original algorithm is computing the main path when creating a new piece, which consumes around 80% of execution time in some cases. To elaborate, the algorithm needs (1) to find a small set of seed voxels <code>SeedPath</code> that is movable in the target direction, (2) add a set of block voxels <code>BlockPath</code> to prevent movement in other directions, and (3) add voxels with small reachability to extend the piece. When computing the start and end of the paths, BFS is used, we believe that this is the root cause of the bottleneck.</p>"},{"location":"milestone_status_report/#further-steps","title":"Further Steps","text":"<p>We are currently discussing optimizing the bottleneck as well as other parts of the algorithm. One possible way is to replace the path generation part with a neural network. No further conclusion has been made yet, but we are actively working on it.</p>"},{"location":"milestone_status_report/#additional-resources","title":"Additional Resources","text":"<ul> <li>Milestone Report Slides</li> <li>Milestone Report Video</li> </ul>"},{"location":"Notes/%5BInt%5DDataset/","title":"Dataset","text":"<p>Date: 2024/04/28 Author: Ruhao Tian</p>"},{"location":"Notes/%5BInt%5DDataset/#dataset-overview","title":"Dataset Overview","text":"<p>\u6570\u636e\u96c6\u751f\u6210\u811a\u672c\u4f4d\u4e8e<code>/src</code>\u76ee\u5f55\u4e0b\u3002</p> <pre><code>\ud83d\udce6Dataset\n \u2523 \ud83d\udcdcDataset.cpp\n \u2517 \ud83d\udcdcDataset.h\n</code></pre> <p>\u4e3b\u8981\u529f\u80fd\u662f\u8bb0\u5f55SeedPath\u548cBlockPath\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u628a\u6700\u7ec8\u7ed3\u679c\u6309\u7167json\u683c\u5f0f\u4fdd\u5b58\u3002</p>"},{"location":"Notes/%5BInt%5DDataset/#_1","title":"\u7f16\u8bd1","text":"<p>\u6240\u6709\u7684\u4ee3\u7801\u90fd\u5b9a\u4e49\u5728<code>DATASET_ENABLE</code>\u5b8f\u4e0b\uff0ccmake\u4f7f\u7528<code>-DENABLE_DATASET=ON</code>\u6765\u5f00\u542f\u3002 \u6839\u76ee\u5f55\u4e0b\u7684<code>compiler_dataset.sh</code>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u8303\u3002</p>"},{"location":"Notes/%5BInt%5DDataset/#_2","title":"\u4f7f\u7528","text":"<p>\u6bcf\u6b21\u4e00\u4e2aSeedPath\u751f\u6210\u5b8c\u6bd5\u540e\uff0c\u5982\u679cSeedPath\u5408\u6cd5\uff0c\u5c31\u4f1a\u88ab\u6dfb\u52a0\u5230\u6267\u884c\u7a0b\u5e8f\u540c\u76ee\u5f55\u4e0b\u7684<code>seedpath.json</code>\u4e2d\uff0c\u5982\u679c\u8fd9\u4e2a\u6587\u4ef6\u5df2\u7ecf\u5b58\u5728\uff0c\u4e0d\u4f1a\u8986\u5199\u5176\u4e2d\u7684\u6570\u636e\u3002\u5982\u679cBlockPath\u5408\u6cd5\uff0c\u5c31\u4f1a\u88ab\u6dfb\u52a0\u5230<code>blockpath.json</code>\u4e2d\u3002</p> <p>\u4e00\u4e2a\u5178\u578b\u7684\u7528\u6cd5\u662f\u8fd0\u884c\u5b8c\u7a0b\u5e8f\u540e\u66f4\u6539\u6587\u4ef6\u540d\u3002<code>/bin/dataset.sh</code>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u8303\u3002</p> <pre><code># Test Dataset\nmeshName=Cube_4x4x4_E1\npathToMesh=../volume/tbl_2_cubes_compare/$meshName.vol\npieceNumber=4\ndifficulty=4\nseed=50\n\n../bin/High-LevelPuzzle_perf $pathToMesh $pieceNumber $difficulty $seed\n\n# rename the output file\n# rename seedpath.json to seedpath_$meshName.json\nmv seedpath.json seedpath_$meshName.json\n# rename blockpath.json to blockpath_$meshName.json\nmv blockpath.json blockpath_$meshName.json\n</code></pre>"},{"location":"Notes/%5BInt%5DDataset/#_3","title":"\u539f\u7406","text":"<p>\u4ee5SeedPath\u4e3a\u4f8b\uff0c\u5728<code>/src/Puzzle/PieceCreator.cpp</code>\uff0c\u5f53<code>SeedPath</code>\u7ed3\u6784\u4f53\u88ab\u521b\u5efa\u65f6\uff0c\u5bf9\u5e94\u7684<code>seedPathCreationSequence</code>\u88ab\u521b\u5efa\uff0c\u6bcf\u5f53<code>SeedPath</code>\u7684\u72b6\u6001\u53d1\u751f\u6539\u53d8\u65f6\uff0c\u66f4\u6539\u540e\u7684<code>SeedPath</code>\u4f1a\u88ab\u6dfb\u52a0\u5230<code>seedPathCreationSequence</code>\u4e2d\u3002\u6700\u7ec8\uff0c<code>seedPathCreationSequence</code>\u4f1a\u88ab\u4fdd\u5b58\u5230json\u6587\u4ef6\u4e2d\u3002</p> <p><code>PieceCreator::CreateSeedPath</code>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u8303\u3002\u9664\u4e86<code>SeedPath</code>\u4ee5\u5916\u7684\u5176\u4ed6\u4e2d\u95f4\u53d8\u91cf\u5747\u4e0d\u4f1a\u88ab\u8bb0\u5f55\u3002</p>"},{"location":"Notes/%5BInt%5DSome_Concerns/","title":"Some Concerns","text":"<p>Date: 2024/04/23 Author: Ruhao Tian</p>"},{"location":"Notes/%5BInt%5DSome_Concerns/#_1","title":"\u5173\u4e8e\u72b6\u6001\u8bb0\u5f55\u7684\u9897\u7c92\u5ea6","text":"<p>\u6211\u4eec\u7684SeedPath\u6570\u636e\u96c6\u6bcf\u4e2a\u9879\u76ee\u662f\u7531\u4e00\u7cfb\u5217<code>SeedPath</code>\u7ed3\u6784\u4f53\u7ec4\u6210\u7684\uff0c\u6bcf\u4e2a\u9879\u76ee\u8bb0\u5f55\u7740SeedPath\u5728\u751f\u6210\u4e2d\u7684\u4e00\u4e2a\u4e0d\u540c\u72b6\u6001\u3002\u6362\u800c\u8a00\u4e4b\u6211\u4eec\u53ea\u6709\u5f53<code>SeedPath</code>\u53d1\u751f\u6539\u53d8\u65f6\u624d\u8bb0\u5f55\u4e00\u4e2a\u65b0\u7684\u72b6\u6001\u4fe1\u606f\uff0c\u4f46\u662f\u5728\u4e24\u4e2a\u72b6\u6001\u4e4b\u95f4\u5176\u5b9e\u6709\u5f88\u591a\u7684\u5b50\u72b6\u6001\uff0c\u5176\u4e2d\u6d89\u53ca\u5f88\u591a\u4e34\u65f6\u53d8\u91cf\u5e76\u6ca1\u6709\u88ab\u8bb0\u5f55\u3002</p> <p>\u6bd4\u5982\uff0c\u627e\u5230SeedPath\u8d77\u59cbVoxel\u548c\u751f\u6210\u5b8c\u6574SeedPath\u662f\u4e24\u4e2a\u76f8\u90bb\u7684\u72b6\u6001\u3002\u4f46\u662f\u4ece\u627e\u5230\u8d77\u59cbVoxel\u5f00\u59cb\uff0c\u8fd8\u8981\u7ecf\u5386\u627e\u5782\u76f4\u79fb\u52a8\u65b9\u5411<code>perpAxisIDs</code>\uff0c\u627e\u76f8\u90bbVoxel<code>neiborVoxelList</code>\uff0c\u7b49\u4e00\u7cfb\u5217\u72b6\u6001\u624d\u80fd\u5230\u8fbe\u751f\u6210\u5b8c\u6574SeedPath\u3002\u7ecf\u5386\u8fd9\u4e9b\u4e2d\u95f4\u72b6\u6001\u65f6<code>SeedPath</code>\u672c\u8eab\u6ca1\u6709\u6539\u53d8\uff0c\u6240\u4ee5\u4ee5\u4e0a\u8fd9\u4e9b\u4e2d\u95f4\u53d8\u91cf\u4e5f\u6ca1\u6709\u88ab\u8bb0\u5f55\u3002</p> <p>\u6211\u62c5\u5fc3\u7684\u662f\u4ec5\u4ec5\u8bb0\u5f55<code>SeedPath</code>\u9897\u7c92\u5ea6\u8fc7\u5927\uff0c\u4e22\u5931\u4e86\u4e00\u4e9b\u53ef\u80fd\u5bf9\u673a\u5668\u5b66\u4e60\u6709\u7528\u7684\u542f\u53d1\u6027\u4fe1\u606f\u3002</p>"},{"location":"Notes/%5BInt%5DSome_Concerns/#reachability","title":"\u5173\u4e8eReachability","text":"<p>\u6211\u4eec\u7684SeedPath\u6570\u636e\u96c6\u5e76\u6ca1\u6709\u8003\u8651\u5230Reachability\uff0c\u4e8b\u5b9e\u4e0a\u4f5c\u8005\u7684\u751f\u6210\u65b9\u6cd5\u4e5f\u6ca1\u6709\u76f4\u63a5\u8003\u8651\u5230Reachability\uff08\u56e0\u4e3a\u6b63\u5411\u8003\u8651Reachability\u5f88\u96be\uff09\u3002\u4f5c\u8005\u7684\u529e\u6cd5\u662f\u91cd\u590d\u968f\u673a\u751f\u6210\u8fc7\u7a0b6\u6b21\u5f97\u5230\u4e00\u4e2a\u5206\u5e03\uff0c\u7136\u540e\u53d6\u5176\u4e2dReachability\u6700\u5c0f\u7684SeedPath\u3002\u6309\u7167\u6211\u4eec\u76ee\u524d\u6570\u636e\u96c6\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u4e5f\u8981\u88ab\u91cd\u590d\u6267\u884c6\u6b21\u3002</p> <p>\u6211\u7684\u60f3\u6cd5\u662f\u6211\u4eec\u628aReachability\u7eb3\u5165\u6a21\u578bReward\u7684\u8ba1\u7b97\u4e2d\uff0c\u8fd9\u6837\u6211\u4eec\u7684\u6a21\u578b\u5c31\u53ef\u4ee5\u76f4\u63a5\u751f\u6210\u4e00\u4e2a\u5177\u6709\u8f83\u5c0fReachability\u7684SeedPath\uff0c\u4ece\u800c\u53ea\u8981\u6267\u884c\u4e00\u6b21\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u3002</p>"}]}